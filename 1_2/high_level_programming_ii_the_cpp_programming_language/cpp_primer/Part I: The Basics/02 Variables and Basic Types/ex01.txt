What are the differences between int, long, long long, and short?
Between an unsigned and a signed type?
Between a float and a double?

short       ->  16 bits (at least)
int         ->  16 bits (at least)
long        ->  32 bits (at least)
long long   ->  64 bits (at least)

unsigned    ->  cannot represent negative values
signed      ->  can represent negative values and positive values

float       ->  6 significant digits,   32 bits,    single-precision
double      ->  10 significant digits,  64 bits,    double-precision
